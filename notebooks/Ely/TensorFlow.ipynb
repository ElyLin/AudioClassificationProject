{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65180abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65261586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c751cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image according to tensorflow's tutorial \n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ba61714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_OptionsDataset element_spec={'image': TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}>\n"
     ]
    }
   ],
   "source": [
    "#  Set Data_dir to the path you want the dataset to be downloaded to. example \"user/desktop/AudioClassificationProject/Data\"\n",
    "# Download is equal to False by default so set to True to download dataset to desired PATH\n",
    "# if you set as_supervised to True then  when calling the dataset it returns a tuple (features, label) instead of a Dictionary \n",
    "# features will be the images and labels is the class \n",
    "# This ended up fixing the iterator warning that was being outputted before, not sure why \n",
    "\n",
    "\n",
    "\n",
    "ds=tfds.load('mnist',\n",
    "    split='train',\n",
    "    shuffle_files=True,\n",
    "    data_dir='/Users/stephen/Desktop/AudioClassificationProject/Data/',  # set PATH for download \n",
    "    download=True,     # set download to True to download dataset locally \n",
    "    #as_supervised=True      #  add so Tuple is returned (features, Label)\n",
    "    )   \n",
    "assert isinstance(ds, tf.data.Dataset)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f7206d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image', 'label']\n",
      "(28, 28, 1) tf.Tensor(4, shape=(), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-06 18:44:48.241875: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# iterate over the dataset\n",
    "ds_sample=ds.take(1) # take a single example\n",
    "for example in ds_sample:\n",
    "    print(list(example.keys()))\n",
    "    image = example['image']\n",
    "    label = example['label']\n",
    "    print(image.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize dataset\n",
    "#ds, info = tfds.load('mnist', split='train', with_info=True)\n",
    "#tfds.as_dataframe(ds.take(4), info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e265805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_OptionsDataset element_spec={'image': TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0790dc",
   "metadata": {},
   "source": [
    "how do I load the data into this notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9dc52187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files belonging to 1 classes.\n",
      "Using 0 files for training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No images found in directory /Users/stephen/Desktop/AudioClassificationProject/Data/mnist/. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m3/ywsrz7k170vbk8511l1h_6840000gn/T/ipykernel_50044/1245203625.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m train_ds = tf.keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/audio_env/lib/python3.9/site-packages/keras/preprocessing/image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m       image_paths, labels, validation_split, subset)\n\u001b[1;32m    208\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     raise ValueError(f'No images found in directory {directory}. '\n\u001b[0m\u001b[1;32m    210\u001b[0m                      f'Allowed formats: {ALLOWLIST_FORMATS}')\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No images found in directory /Users/stephen/Desktop/AudioClassificationProject/Data/mnist/. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')"
     ]
    }
   ],
   "source": [
    "# in order to create a Dataset object from .image_dataset_from_directory function you must have the dataset downloaded\n",
    "# data_dir will be a string that represents the PATH of the dataset master folder. \n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_dir = '/Users/stephen/Desktop/AudioClassificationProject/Data/mnist/'\n",
    "image_size = (28, 28)\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=image_size,\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8297ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
